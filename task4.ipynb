{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9e4ee0",
   "metadata": {},
   "source": [
    "## Regression Tree, Random Forest and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5265f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b82ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "df = pd.read_csv('data/training_data_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24baee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les variables clés\n",
    "key_features = ['PM2.5', 'PM10', 'O3', 'NO2', 'CO', 'WindSpeed', \n",
    "                'SO2', 'NOx', 'NO', 'O3_8hr', 'CO_8hr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2842765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le dataset\n",
    "data = df[key_features + ['AQI']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48e1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division train/test\n",
    "X = data[key_features]\n",
    "y = data['AQI']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c6f8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TREE-BASED METHODS : Style ISLP\n",
      "======================================================================\n",
      "\n",
      "Taille d'entraînement: (56633, 11)\n",
      "Taille de test: (14159, 11)\n",
      "Variables utilisées: 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TREE-BASED METHODS : Style ISLP\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTaille d'entraînement: {X_train.shape}\")\n",
    "print(f\"Taille de test: {X_test.shape}\")\n",
    "print(f\"Variables utilisées: {len(key_features)}\\n\")\n",
    "\n",
    "# Stockage des résultats\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc153f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "1. REGRESSION TREES\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# 1. REGRESSION TREES\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"1. REGRESSION TREES\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fcf597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1 Arbre complet (unpruned)\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Arbre complet (unpruned)\n",
    "print(\"\\n1.1 Arbre complet (unpruned)\")\n",
    "tree_full = DecisionTreeRegressor(random_state=1)\n",
    "tree_full.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_train_pred = tree_full.predict(X_train)\n",
    "y_test_pred = tree_full.predict(X_test)\n",
    "\n",
    "# Erreurs\n",
    "mse_train_full = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test_full = mean_squared_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b74788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nombre de feuilles: 51084\n",
      "  Profondeur: 38\n",
      "  MSE Train: 0.0000\n",
      "  MSE Test:  315.8875\n"
     ]
    }
   ],
   "source": [
    "print(f\"  Nombre de feuilles: {tree_full.get_n_leaves()}\")\n",
    "print(f\"  Profondeur: {tree_full.get_depth()}\")\n",
    "print(f\"  MSE Train: {mse_train_full:.4f}\")\n",
    "print(f\"  MSE Test:  {mse_test_full:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5ffc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.2 Cost Complexity Pruning\n",
      "  Nombre de alphas: 46188\n",
      "\n",
      "  Alpha optimal: 0.012384\n",
      "  MSE Test optimal: 241.8098\n",
      "  Nombre de feuilles: 3674\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Pruning: Tester différentes profondeurs max\n",
    "print(\"\\n1.2 Cost Complexity Pruning\")\n",
    "\n",
    "# Obtenir le chemin de complexité\n",
    "path = tree_full.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "impurities = path.impurities\n",
    "\n",
    "print(f\"  Nombre de alphas: {len(ccp_alphas)}\")\n",
    "\n",
    "# Limiter le nombre d'alphas pour éviter le surapprentissage\n",
    "ccp_alphas = ccp_alphas[::len(ccp_alphas)//20]  # Prendre ~20 valeurs\n",
    "\n",
    "# Entraîner des arbres pour chaque alpha\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "n_leaves = []\n",
    "\n",
    "for alpha in ccp_alphas:\n",
    "    tree = DecisionTreeRegressor(random_state=1, ccp_alpha=alpha)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_scores.append(mean_squared_error(y_train, tree.predict(X_train)))\n",
    "    test_scores.append(mean_squared_error(y_test, tree.predict(X_test)))\n",
    "    n_leaves.append(tree.get_n_leaves())\n",
    "\n",
    "# Meilleur alpha\n",
    "best_idx = np.argmin(test_scores)\n",
    "best_alpha = ccp_alphas[best_idx]\n",
    "\n",
    "print(f\"\\n  Alpha optimal: {best_alpha:.6f}\")\n",
    "print(f\"  MSE Test optimal: {test_scores[best_idx]:.4f}\")\n",
    "print(f\"  Nombre de feuilles: {n_leaves[best_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de4f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Top 5 variables importantes:\n"
     ]
    }
   ],
   "source": [
    "# Ajuster l'arbre optimal\n",
    "tree_pruned = DecisionTreeRegressor(random_state=1, ccp_alpha=best_alpha)\n",
    "tree_pruned.fit(X_train, y_train)\n",
    "y_pred_tree = tree_pruned.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Méthode': f'Regression Tree (pruned)',\n",
    "    'MSE Train': mean_squared_error(y_train, tree_pruned.predict(X_train)),\n",
    "    'MSE Test': test_scores[best_idx],\n",
    "    'RMSE Test': np.sqrt(test_scores[best_idx])\n",
    "})\n",
    "\n",
    "# Variable importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Variable': key_features,\n",
    "    'Importance': tree_pruned.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n  Top 5 variables importantes:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a87c60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Top 5 variables importantes:\n",
      "    PM2.5: 0.8348\n",
      "    O3_8hr: 0.0588\n",
      "    PM10: 0.0332\n",
      "    CO_8hr: 0.0128\n",
      "    CO: 0.0114\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n  Top 5 variables importantes:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"    {row['Variable']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb27fdc",
   "metadata": {},
   "source": [
    "### BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a5048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "2. BAGGING\n",
      "======================================================================\n",
      "\n",
      "Testing different numbers of trees...\n",
      "  n_trees= 10: MSE Test = 171.2346\n",
      "  n_trees= 50: MSE Test = 159.0321\n",
      "  n_trees=100: MSE Test = 157.6187\n",
      "  n_trees=200: MSE Test = 157.2776\n",
      "  n_trees=500: MSE Test = 156.9320\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2. BAGGING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bagging = Random Forest avec max_features = p (toutes les variables)\n",
    "# Tester différents nombres d'arbres\n",
    "n_trees_list = [10, 50, 100, 200, 500]\n",
    "bagging_mse_test = []\n",
    "\n",
    "print(\"\\nTesting different numbers of trees...\")\n",
    "for n_trees in n_trees_list:\n",
    "    # max_features=None utilise toutes les features (= Bagging)\n",
    "    rf = RandomForestRegressor(n_estimators=n_trees, \n",
    "                                max_features=None,\n",
    "                                random_state=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    bagging_mse_test.append(mse_test)\n",
    "    \n",
    "    print(f\"  n_trees={n_trees:3d}: MSE Test = {mse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b506a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Optimal: 500 arbres (MSE Test: 156.9320)\n"
     ]
    }
   ],
   "source": [
    "# Meilleur nombre d'arbres\n",
    "best_n_trees_bag = n_trees_list[np.argmin(bagging_mse_test)]\n",
    "print(f\"\\n→ Optimal: {best_n_trees_bag} arbres (MSE Test: {min(bagging_mse_test):.4f})\")\n",
    "\n",
    "# Ajuster le meilleur modèle\n",
    "rf_bagging = RandomForestRegressor(n_estimators=best_n_trees_bag,\n",
    "                                    max_features=None,\n",
    "                                    random_state=1)\n",
    "rf_bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = rf_bagging.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Méthode': f'Bagging (B={best_n_trees_bag})',\n",
    "    'MSE Train': mean_squared_error(y_train, rf_bagging.predict(X_train)),\n",
    "    'MSE Test': min(bagging_mse_test),\n",
    "    'RMSE Test': np.sqrt(min(bagging_mse_test))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31faa338",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cf12be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3. RANDOM FORESTS\n",
      "======================================================================\n",
      "\n",
      "3.1 Optimisation de max_features (m)\n",
      "  m= 3 (sqrt(p)): MSE Test = 154.2325\n",
      "  m= 3 (p/3   ): MSE Test = 154.2325\n",
      "  m= 5 (p/2   ): MSE Test = 154.2998\n",
      "  m=11 (p     ): MSE Test = 156.9320\n",
      "\n",
      "→ Optimal: m=3 (MSE Test: 154.2325)\n",
      "\n",
      "3.2 Optimisation du nombre d'arbres (B) avec m=3\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# 3. RANDOM FORESTS\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3. RANDOM FORESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 3.1 Tester différents max_features (m)\n",
    "print(\"\\n3.1 Optimisation de max_features (m)\")\n",
    "\n",
    "# Valeurs typiques: sqrt(p), p/3, p/2\n",
    "p = len(key_features)\n",
    "m_values = [int(np.sqrt(p)), p//3, p//2, p]\n",
    "m_labels = ['sqrt(p)', 'p/3', 'p/2', 'p']\n",
    "\n",
    "rf_mse_test = []\n",
    "\n",
    "for m, label in zip(m_values, m_labels):\n",
    "    rf = RandomForestRegressor(n_estimators=500, \n",
    "                                max_features=m,\n",
    "                                random_state=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rf_mse_test.append(mse_test)\n",
    "    \n",
    "    print(f\"  m={m:2d} ({label:6s}): MSE Test = {mse_test:.4f}\")\n",
    "\n",
    "# Meilleur m\n",
    "best_m_idx = np.argmin(rf_mse_test)\n",
    "best_m = m_values[best_m_idx]\n",
    "print(f\"\\n→ Optimal: m={best_m} (MSE Test: {rf_mse_test[best_m_idx]:.4f})\")\n",
    "\n",
    "# 3.2 Tester le nombre d'arbres avec le meilleur m\n",
    "print(f\"\\n3.2 Optimisation du nombre d'arbres (B) avec m={best_m}\")\n",
    "\n",
    "n_trees_list_rf = [50, 100, 200, 300, 500, 1000]\n",
    "rf_ntrees_mse_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47bd2669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  B=  50: MSE Test = 157.6697\n",
      "  B= 100: MSE Test = 155.9251\n",
      "  B= 200: MSE Test = 154.7931\n",
      "  B= 300: MSE Test = 154.4789\n",
      "  B= 500: MSE Test = 154.2325\n",
      "  B=1000: MSE Test = 153.9998\n",
      "\n",
      "→ Optimal: B=1000 (MSE Test: 153.9998)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 8388608 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Ajuster le meilleur Random Forest\u001b[39;00m\n\u001b[32m     18\u001b[39m rf_best = RandomForestRegressor(n_estimators=best_n_trees_rf,\n\u001b[32m     19\u001b[39m                                  max_features=best_m,\n\u001b[32m     20\u001b[39m                                  random_state=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mrf_best\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m y_pred_rf = rf_best.predict(X_test)\n\u001b[32m     24\u001b[39m results.append({\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMéthode\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mRandom Forest (B=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_n_trees_rf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMSE Train\u001b[39m\u001b[33m'\u001b[39m: mean_squared_error(y_train, rf_best.predict(X_train)),\n\u001b[32m     27\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMSE Test\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mmin\u001b[39m(rf_ntrees_mse_test),\n\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRMSE Test\u001b[39m\u001b[33m'\u001b[39m: np.sqrt(\u001b[38;5;28mmin\u001b[39m(rf_ntrees_mse_test))\n\u001b[32m     29\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AIMS\\miniconda3\\envs\\islp\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AIMS\\miniconda3\\envs\\islp\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    478\u001b[39m trees = [\n\u001b[32m    479\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    480\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    481\u001b[39m ]\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AIMS\\miniconda3\\envs\\islp\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     62\u001b[39m config = get_config()\n\u001b[32m     63\u001b[39m iterable_with_config = (\n\u001b[32m     64\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     66\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AIMS\\miniconda3\\envs\\islp\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AIMS\\miniconda3\\envs\\islp\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AIMS\\miniconda3\\envs\\islp\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m     config = {}\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AIMS\\miniconda3\\envs\\islp\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[39m, in \u001b[36m_parallel_build_trees\u001b[39m\u001b[34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    190\u001b[39m         curr_sample_weight *= compute_sample_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, y, indices=indices)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    200\u001b[39m     tree._fit(\n\u001b[32m    201\u001b[39m         X,\n\u001b[32m    202\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m         missing_values_in_feature_mask=missing_values_in_feature_mask,\n\u001b[32m    206\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AIMS\\miniconda3\\envs\\islp\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_tree.pyx:172\u001b[39m, in \u001b[36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_tree.pyx:287\u001b[39m, in \u001b[36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_tree.pyx:942\u001b[39m, in \u001b[36msklearn.tree._tree.Tree._add_node\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_tree.pyx:910\u001b[39m, in \u001b[36msklearn.tree._tree.Tree._resize_c\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_utils.pyx:35\u001b[39m, in \u001b[36msklearn.tree._utils.safe_realloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: could not allocate 8388608 bytes"
     ]
    }
   ],
   "source": [
    "for n_trees in n_trees_list_rf:\n",
    "    rf = RandomForestRegressor(n_estimators=n_trees,\n",
    "                                max_features=best_m,\n",
    "                                random_state=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rf_ntrees_mse_test.append(mse_test)\n",
    "    \n",
    "    print(f\"  B={n_trees:4d}: MSE Test = {mse_test:.4f}\")\n",
    "\n",
    "# Meilleur nombre d'arbres\n",
    "best_n_trees_rf = n_trees_list_rf[np.argmin(rf_ntrees_mse_test)]\n",
    "print(f\"\\n→ Optimal: B={best_n_trees_rf} (MSE Test: {min(rf_ntrees_mse_test):.4f})\")\n",
    "\n",
    "# Ajuster le meilleur Random Forest\n",
    "rf_best = RandomForestRegressor(n_estimators=best_n_trees_rf,\n",
    "                                 max_features=best_m,\n",
    "                                 random_state=1)\n",
    "rf_best.fit(X_train, y_train)\n",
    "y_pred_rf = rf_best.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Méthode': f'Random Forest (B={best_n_trees_rf}, m={best_m})',\n",
    "    'MSE Train': mean_squared_error(y_train, rf_best.predict(X_train)),\n",
    "    'MSE Test': min(rf_ntrees_mse_test),\n",
    "    'RMSE Test': np.sqrt(min(rf_ntrees_mse_test))\n",
    "})\n",
    "\n",
    "# Variable importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Variable': key_features,\n",
    "    'Importance': rf_best.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n  Top 5 variables importantes:\")\n",
    "for idx, row in rf_importance.head(5).iterrows():\n",
    "    print(f\"    {row['Variable']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff787d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "628aca88",
   "metadata": {},
   "source": [
    "## Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4086ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "4. BOOSTING (Gradient Boosting)\n",
      "======================================================================\n",
      "\n",
      "4.1 Optimisation de la profondeur (max_depth)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4. BOOSTING (Gradient Boosting)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 4.1 Tester différentes profondeurs d'arbres\n",
    "print(\"\\n4.1 Optimisation de la profondeur (max_depth)\")\n",
    "\n",
    "depths = [1, 2, 3, 4, 5]\n",
    "boost_depth_mse_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d59644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  depth=1: MSE Test = 227.2746\n",
      "  depth=2: MSE Test = 183.3081\n",
      "  depth=3: MSE Test = 168.7318\n",
      "  depth=4: MSE Test = 161.5213\n"
     ]
    }
   ],
   "source": [
    "for depth in depths:\n",
    "    gbm = GradientBoostingRegressor(n_estimators=500,\n",
    "                                     max_depth=depth,\n",
    "                                     learning_rate=0.01,\n",
    "                                     random_state=1)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = gbm.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    boost_depth_mse_test.append(mse_test)\n",
    "    \n",
    "    print(f\"  depth={depth}: MSE Test = {mse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meilleure profondeur\n",
    "best_depth = depths[np.argmin(boost_depth_mse_test)]\n",
    "print(f\"\\n→ Optimal depth: {best_depth} (MSE Test: {min(boost_depth_mse_test):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Tester différents learning rates\n",
    "print(f\"\\n4.2 Optimisation du learning rate (λ) avec depth={best_depth}\")\n",
    "\n",
    "lambdas = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "boost_lambda_mse_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lam in lambdas:\n",
    "    gbm = GradientBoostingRegressor(n_estimators=500,\n",
    "                                     max_depth=best_depth,\n",
    "                                     learning_rate=lam,\n",
    "                                     random_state=1)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = gbm.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    boost_lambda_mse_test.append(mse_test)\n",
    "    \n",
    "    print(f\"  λ={lam:.3f}: MSE Test = {mse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meilleur learning rate\n",
    "best_lambda = lambdas[np.argmin(boost_lambda_mse_test)]\n",
    "print(f\"\\n→ Optimal λ: {best_lambda} (MSE Test: {min(boost_lambda_mse_test):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Tester le nombre d'itérations avec les meilleurs paramètres\n",
    "print(f\"\\n4.3 Optimisation de B avec depth={best_depth}, λ={best_lambda}\")\n",
    "\n",
    "n_trees_boost = [100, 200, 500, 1000, 2000]\n",
    "boost_ntrees_mse_test = []\n",
    "boost_train_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_trees in n_trees_boost:\n",
    "    gbm = GradientBoostingRegressor(n_estimators=n_trees,\n",
    "                                     max_depth=best_depth,\n",
    "                                     learning_rate=best_lambda,\n",
    "                                     random_state=1)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = gbm.predict(X_train)\n",
    "    y_test_pred = gbm.predict(X_test)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    boost_train_errors.append(mse_train)\n",
    "    boost_ntrees_mse_test.append(mse_test)\n",
    "    \n",
    "    print(f\"  B={n_trees:4d}: MSE Train = {mse_train:.4f}, MSE Test = {mse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meilleur nombre d'arbres\n",
    "best_n_trees_boost = n_trees_boost[np.argmin(boost_ntrees_mse_test)]\n",
    "print(f\"\\n→ Optimal: B={best_n_trees_boost} (MSE Test: {min(boost_ntrees_mse_test):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99668181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuster le meilleur modèle de boosting\n",
    "gbm_best = GradientBoostingRegressor(n_estimators=best_n_trees_boost,\n",
    "                                      max_depth=best_depth,\n",
    "                                      learning_rate=best_lambda,\n",
    "                                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_best.fit(X_train, y_train)\n",
    "y_pred_boost = gbm_best.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'Méthode': f'Boosting (B={best_n_trees_boost}, d={best_depth}, λ={best_lambda})',\n",
    "    'MSE Train': mean_squared_error(y_train, gbm_best.predict(X_train)),\n",
    "    'MSE Test': min(boost_ntrees_mse_test),\n",
    "    'RMSE Test': np.sqrt(min(boost_ntrees_mse_test))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance\n",
    "boost_importance = pd.DataFrame({\n",
    "    'Variable': key_features,\n",
    "    'Importance': gbm_best.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd531e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n  Top 5 variables importantes:\")\n",
    "for idx, row in boost_importance.head(5).iterrows():\n",
    "    print(f\"    {row['Variable']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d53978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 5. COMPARAISON DES MÉTHODES\n",
    "# ====================================================================\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('MSE Test')\n",
    "\n",
    "print(\"\\n\", results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149094f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_method = results_df.iloc[0]['Méthode']\n",
    "best_mse = results_df.iloc[0]['MSE Test']\n",
    "print(f\"\\n→ Meilleure méthode: {best_method}\")\n",
    "print(f\"  MSE Test: {best_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer avec l'arbre simple\n",
    "improvement = (mse_test_full - best_mse) / mse_test_full * 100\n",
    "print(f\"  Amélioration vs arbre simple: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bf431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 6. VISUALISATIONS (Style ISLP)\n",
    "# ====================================================================\n",
    "\n",
    "# Figure 1: Pruning du regression tree\n",
    "fig, axes = subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(ccp_alphas, train_scores, marker='o', label='Train', drawstyle='steps-post')\n",
    "axes[0].plot(ccp_alphas, test_scores, marker='s', label='Test', drawstyle='steps-post')\n",
    "axes[0].axvline(best_alpha, color='red', linestyle='--', label=f'Optimal α={best_alpha:.4f}')\n",
    "axes[0].set_xlabel('alpha')\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].set_title('Cost Complexity Pruning')\n",
    "axes[0].legend()\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "axes[1].plot(n_leaves, test_scores, marker='o', color='green')\n",
    "axes[1].set_xlabel('Number of leaves')\n",
    "axes[1].set_ylabel('MSE Test')\n",
    "axes[1].set_title('MSE vs Tree Size')\n",
    "axes[1].axvline(n_leaves[best_idx], color='red', linestyle='--')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Comparaison Bagging vs Random Forest vs Boosting\n",
    "fig, axes = subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Bagging\n",
    "axes[0, 0].plot(n_trees_list, bagging_mse_test, marker='o', lw=2)\n",
    "axes[0, 0].set_xlabel('Number of Trees (B)')\n",
    "axes[0, 0].set_ylabel('MSE Test')\n",
    "axes[0, 0].set_title('Bagging')\n",
    "axes[0, 0].axhline(min(bagging_mse_test), color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Random Forest - max_features\n",
    "axes[0, 1].plot(m_values, rf_mse_test, marker='s', color='green', lw=2)\n",
    "axes[0, 1].set_xlabel('max_features (m)')\n",
    "axes[0, 1].set_ylabel('MSE Test')\n",
    "axes[0, 1].set_title('Random Forest: max_features')\n",
    "axes[0, 1].set_xticks(m_values)\n",
    "axes[0, 1].set_xticklabels(m_labels)\n",
    "\n",
    "# Random Forest - nombre d'arbres\n",
    "axes[1, 0].plot(n_trees_list_rf, rf_ntrees_mse_test, marker='o', color='orange', lw=2)\n",
    "axes[1, 0].set_xlabel('Number of Trees (B)')\n",
    "axes[1, 0].set_ylabel('MSE Test')\n",
    "axes[1, 0].set_title(f'Random Forest (m={best_m})')\n",
    "axes[1, 0].axhline(min(rf_ntrees_mse_test), color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Boosting\n",
    "axes[1, 1].plot(n_trees_boost, boost_train_errors, marker='o', label='Train', lw=2)\n",
    "axes[1, 1].plot(n_trees_boost, boost_ntrees_mse_test, marker='s', label='Test', lw=2)\n",
    "axes[1, 1].set_xlabel('Number of Trees (B)')\n",
    "axes[1, 1].set_ylabel('MSE')\n",
    "axes[1, 1].set_title(f'Boosting (depth={best_depth}, λ={best_lambda})')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Variable Importance Comparison\n",
    "fig, axes = subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Tree\n",
    "axes[0].barh(feature_importance['Variable'].head(10), \n",
    "             feature_importance['Importance'].head(10))\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Regression Tree')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Random Forest\n",
    "axes[1].barh(rf_importance['Variable'].head(10), \n",
    "             rf_importance['Importance'].head(10), color='green')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Random Forest')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Boosting\n",
    "axes[2].barh(boost_importance['Variable'].head(10), \n",
    "             boost_importance['Importance'].head(10), color='orange')\n",
    "axes[2].set_xlabel('Importance')\n",
    "axes[2].set_title('Boosting')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e59d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Visualization de l'arbre pruned (si pas trop grand)\n",
    "if tree_pruned.get_n_leaves() <= 20:\n",
    "    fig, ax = subplots(figsize=(20, 10))\n",
    "    plot_tree(tree_pruned, \n",
    "              feature_names=key_features,\n",
    "              filled=True,\n",
    "              rounded=True,\n",
    "              fontsize=10,\n",
    "              ax=ax)\n",
    "    ax.set_title('Pruned Regression Tree')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"\\nArbre trop grand pour être visualisé ({tree_pruned.get_n_leaves()} feuilles)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARAISON "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "islp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
